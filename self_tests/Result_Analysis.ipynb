{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68317f85-0ea0-49aa-9da6-44b01ebe5f95",
   "metadata": {},
   "source": [
    "# Analysis of the Results\n",
    "\n",
    "For a given set of tools, automatic test cases are generated. This notebook analyzes the quality of the questions generated along with their diversity and whether they really span the requested combinations of tools.\n",
    "\n",
    "Note that GPT may generate `functions.<function_name>` instead of just the function name and we account for that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c99e2e-b307-49a5-b287-0b7ffb8820a7",
   "metadata": {},
   "source": [
    "## Test case generation\n",
    "\n",
    "We demanded the LLMs to generate test cases including all of their tools. We would like 2 tests per tool (individually) and 2 tests that require exactly 2 of the tools to be answered correctly. Let us verify if this generation was correct.\n",
    "\n",
    "For each test generation strategy, we want to check:\n",
    "\n",
    "- Number of test cases generated per tool (individually);\n",
    "- Number of test cases generated per tool (considering pairs);\n",
    "- How often the model generated questions that used the tool requested;\n",
    "- Manually, how many of the questions really need the tools planned."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb88853-601e-426d-84cc-648d26189b3a",
   "metadata": {},
   "source": [
    "### Set up and read files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb76bb6-e718-4cd0-a5fd-be10994f77d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import self_test_utils as stu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5482153-d59c-49b7-99e3-8545d0ff9007",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gen_strategies = [\n",
    "    'use_all',\n",
    "    'only_selected',\n",
    "    'selected_with_dummies',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0a828a-0ee3-41f1-9445-50ff0eddb749",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_files = [x for x in os.listdir() if x.endswith('.json')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66354874-bac4-4a32-b310-684a505773c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_generated_tests():\n",
    "    df_tests = None\n",
    "    for cur_file in test_files:\n",
    "        with open(cur_file, 'r') as f:\n",
    "            contents = json.loads(f.read())\n",
    "            file_info = cur_file.split('_test_cases_')\n",
    "            df = pd.DataFrame(contents)\n",
    "            df['gen_strategy'] = file_info[0]\n",
    "            df['model'] = file_info[1].replace('.json', '')\n",
    "            if df_tests is None:\n",
    "                df_tests = df\n",
    "            else:\n",
    "                df_tests = pd.concat([df_tests, df])\n",
    "    return df_tests\n",
    "\n",
    "\n",
    "df_tests = read_generated_tests()\n",
    "df_tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffbfffc-6608-4099-86b0-c1fa221417e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute all tools that were tested\n",
    "all_tools = [\n",
    "    x for x in set(df_tests.expected_tool_to_gen_test.dropna()) if ',' not in x\n",
    "]\n",
    "all_tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e038dd7e-0c3a-47eb-a7b8-32d2f9cc3405",
   "metadata": {},
   "source": [
    "### Number of test cases generated per tool (individually)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325be888-9683-4a04-987a-5599fefe8a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a dataframe with boolean flags for each tool\n",
    "df_tests_per_tool = df_tests.copy()\n",
    "for t in all_tools:\n",
    "    df_tests_per_tool[t] = df_tests_per_tool.appropriate_tools.map(lambda z: t in str(z).replace('functions.', ''))\n",
    "\n",
    "cols = ['model', 'gen_strategy'] + all_tools\n",
    "df_tests_per_tool[cols].groupby(by=['model', 'gen_strategy']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f68d985-a8db-48f8-a4cc-4baa748f7b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tests_per_tool[cols].groupby(\n",
    "    by=['model', 'gen_strategy']\n",
    ").sum().plot.barh(figsize=(15, 15), title='Number of test cases generated per tool/strategy')\n",
    "# plt.xticks(rotation = 45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ae9eac-cd54-4276-b23a-5cf48ef4a219",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg = df_tests_per_tool[cols].groupby(by=['model', 'gen_strategy']).sum()\n",
    "df_agg['coverage'] = np.sum(df_agg[all_tools].values > 0, axis=1) / len(all_tools)\n",
    "df_agg = df_agg.sort_values(by='coverage', ascending=False)\n",
    "df_agg[['coverage']].plot.barh(title='Single tool test coverage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d7cdfc-96e3-420e-8c6d-f03ba31307a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# missing coverage\n",
    "all_missing = []\n",
    "for idx, r in df_agg[all_tools].iterrows():\n",
    "    missing_tools = [x for x in all_tools if r[x] == 0]\n",
    "    all_missing.append(missing_tools)\n",
    "df_agg['missing_tools'] = all_missing\n",
    "df_agg[df_agg.coverage < 1][['coverage', 'missing_tools']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc93cb5-dc0f-48ad-b6f9-37eec33d8d9d",
   "metadata": {},
   "source": [
    "## Number of test cases generated per tool (considering pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2869d0c-6b7b-4b0a-ab0b-25621748e611",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290f390b-73af-47f3-9014-b3c2674ed780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a dataframe with boolean flags for tool combinations\n",
    "\n",
    "all_tools_and_pairs = [[x] for x in all_tools]\n",
    "for i, t1 in enumerate(all_tools[0:-1]):\n",
    "    for j, t2 in enumerate(all_tools[i + 1:]):\n",
    "        all_tools_and_pairs.append([t1, t2])\n",
    "\n",
    "df_pair_tests_per_tool = df_tests.copy()\n",
    "df_pair_tests_per_tool['invented_tools'] = df_pair_tests_per_tool.appropriate_tools.map(lambda z: stu.detect_invented_tools(z, all_tools))\n",
    "tool_cols = [','.join(t) for t in all_tools_and_pairs]\n",
    "for t in all_tools_and_pairs:\n",
    "    df_pair_tests_per_tool[','.join(t)] = df_pair_tests_per_tool.appropriate_tools.map(lambda z: stu.belongs_to_col(z, t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3826ad29-5a82-4f56-8c47-95bcbac1a5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_agg.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b40c06f-e3ec-4f5d-ab24-dc7f701ee0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['model', 'gen_strategy'] + tool_cols\n",
    "df_pair_tests_per_tool[cols].groupby(by=['model', 'gen_strategy']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd623c4-2069-4da1-a19b-0d70eebdebaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg = df_pair_tests_per_tool[cols].groupby(by=['model', 'gen_strategy']).sum()\n",
    "df_agg['coverage'] = np.sum(df_agg[tool_cols].values > 0, axis=1) / len(tool_cols)\n",
    "df_agg = df_agg.sort_values(by='coverage', ascending=False)\n",
    "df_agg[['coverage']].plot.barh(title='Single and pairwise tool test coverage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61c5ce0-4ff3-49ff-be4a-a9ac4a091ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# missing coverage and invented tools\n",
    "from IPython.display import display, Math, Latex\n",
    "\n",
    "all_missing = []\n",
    "for idx, r in df_agg[tool_cols].iterrows():\n",
    "    missing_tools = [x for x in tool_cols if r[x] == 0]\n",
    "    all_missing.append(missing_tools)\n",
    "df_agg['missing_tools'] = all_missing\n",
    "\n",
    "latex_tbl = df_agg[df_agg.coverage <= 1][['coverage']].to_latex(float_format=\"{:.2f}\".format,)\n",
    "df_agg[df_agg.coverage <= 1][['coverage', 'missing_tools']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092f7855-fecc-43e9-958a-76bcdfe0ace8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export for paper\n",
    "# print(latex_tbl.replace('_', '\\\\_').replace(' - Anthropic', '').replace(' - OpenAI', '').replace(' - Bedrock', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e50ed73-6c01-484f-9617-e48f04231b41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189a4499-2280-4aa3-b9bf-ba2e1cffc618",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d64d56d-0ee6-4cdd-a2f6-e77b4cb79b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pair_tests_per_tool[df_pair_tests_per_tool['invented_tools'] != ''].groupby(['model', 'gen_strategy']).count()['question']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06057bd9-f5f3-426a-ae83-5ecce340eaa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [stu.detect_invented_tools(x) for x in df_tests.appropriate_tools[20:40]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16a4a2e-8d8d-479d-ab99-dc1aed16482d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set([stu.detect_invented_tools(x) for x in df_tests.appropriate_tools])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b167a331-a1e3-4d6f-9a8e-899ef77a0153",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "64b7f67d-47ae-4f7b-b0f6-efb6e241f8fb",
   "metadata": {},
   "source": [
    "## Manual verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4415770-c241-4e9e-86fd-69d98dc35c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "manually_verified_files = [\n",
    "    'selected_with_dummies_test_cases_GPT 4o - OpenAI.json',\n",
    "    'selected_with_dummies_test_cases_Claude 3.5 Sonnet - Anthropic.json',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332d974a-f8b4-494d-bd51-1c10be790bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_human_answers(df, file):\n",
    "    list_is_correct = []\n",
    "    list_score = []\n",
    "    list_invented_tools = []\n",
    "    list_is_empty_pred = []\n",
    "    for idx, row in df.iterrows():\n",
    "        is_correct, score, invented_tools, is_empty_pred = stu.is_tool_selection_correct(\n",
    "            json.dumps(row['human_verified_tools']),\n",
    "            json.dumps(row['appropriate_tools']),\n",
    "            all_tools\n",
    "        )\n",
    "        list_is_correct.append(is_correct)\n",
    "        list_score.append(score)\n",
    "        list_invented_tools.append(invented_tools)\n",
    "        list_is_empty_pred.append(is_empty_pred)\n",
    "    df['is_correct'] = list_is_correct\n",
    "    df['score'] = list_score\n",
    "    df['invented_tools'] = list_invented_tools\n",
    "    df['is_empty_pred'] = list_is_empty_pred\n",
    "    df['file'] = file\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07683a0f-7d44-466c-8475-01f013c6842b",
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_verif_dfs = None\n",
    "for cur_file in manually_verified_files:\n",
    "    with open(cur_file, 'r') as f:\n",
    "        cur_data = json.loads(f.read())\n",
    "    df = pd.DataFrame(cur_data)\n",
    "    check_human_answers(df, cur_file)\n",
    "    if manual_verif_dfs is None:\n",
    "        manual_verif_dfs = df\n",
    "    else:\n",
    "        manual_verif_dfs = pd.concat([manual_verif_dfs, df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a28c663-7067-443b-ad9d-5c573828f99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_verif_dfs.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e155a50-46ca-478c-910f-8990c0f4ac05",
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_verif_dfs.groupby(by=['file']).agg(\n",
    "    {\n",
    "        'question': ['count'],\n",
    "        'is_correct': ['mean'],\n",
    "        'score': ['mean'],\n",
    "        'is_empty_pred': ['sum'],\n",
    "        # 'n_invented_tools': ['sum'],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c202f065-d01c-48f4-aa43-b55697a4feff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b8575f-4e37-40de-8cbf-64bbecd15b77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d494094f-abdd-49a3-b636-ac24d0fbc297",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "Make sure to test when\n",
    "\n",
    "- No tools were planned\n",
    "- Tools invoked when we explicitly asked not to\n",
    "- Tool names were made up\n",
    "- Correct tools planned\n",
    "- Only one of the correct tools was planned\n",
    "\n",
    "Note that we only evaluate answers given in valid JSON format enclosed within \\<answer>\\</answer> tags, as requested in the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7019841e-278b-4a67-be0c-7bed36057cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_files = [x for x in os.listdir() if x.startswith('self_test_results')]\n",
    "eval_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2de4d5-4083-4998-8c1d-68fcb8b2fc29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bea29e7-b4d2-4b3b-82c3-a0d8719dd695",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval_models = None\n",
    "for cur_eval_file in eval_files:\n",
    "    df = pd.read_csv(cur_eval_file)\n",
    "    if df_eval_models is None:\n",
    "        df_eval_models = df\n",
    "    else:\n",
    "        df_eval_models = pd.concat([df_eval_models, df])\n",
    "\n",
    "# evaluate accuracy\n",
    "is_correct = []\n",
    "invented_tools = []\n",
    "scores = []\n",
    "is_pred_empty = []\n",
    "for idx, r in df_eval_models.iterrows():\n",
    "    cur_correct, cur_score, cur_invented, cur_empty = stu.is_tool_selection_correct(\n",
    "        r['expected_answer'], r['parsed_tool_names'], all_tools, r['model']\n",
    "    )\n",
    "    is_correct.append(cur_correct)\n",
    "    invented_tools.append(cur_invented)\n",
    "    scores.append(cur_score)\n",
    "    is_pred_empty.append(cur_empty)\n",
    "df_eval_models['scores'] = scores\n",
    "df_eval_models['is_correct'] = is_correct\n",
    "df_eval_models['n_invented_tools'] = [len(x) for x in invented_tools]\n",
    "df_eval_models['invented_tools'] = invented_tools\n",
    "df_eval_models['is_pred_empty'] = is_pred_empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959b9f12-2926-437d-95e4-606d4e35aed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541c2617-94e6-460a-bf32-a4935061b568",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_eval = df_eval_models.groupby(by=['model', 'use_native_tools']).agg(\n",
    "    {\n",
    "        'question': ['count'],\n",
    "        'is_correct': ['sum'],\n",
    "        'scores': ['sum'],\n",
    "        'is_pred_empty': ['sum'],\n",
    "        'n_invented_tools': ['sum'],\n",
    "    }\n",
    ")\n",
    "agg_eval[('accuracy', '%')] = agg_eval.values[:,1] / agg_eval.values[:,0]\n",
    "agg_eval[('score', '%')] = agg_eval.values[:,2] / agg_eval.values[:,0]\n",
    "agg_eval = agg_eval.sort_values(by=('score', '%'), ascending=False)\n",
    "agg_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdde44c1-bc42-4af2-b795-a6b2f973cd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import gat_llm.llm_invoker as inv\n",
    "\n",
    "# get model costs\n",
    "inp_prices = []\n",
    "out_prices = []\n",
    "for x in agg_eval.index:\n",
    "    print(x[0])\n",
    "    cur_llm = inv.LLM_Provider.get_llm(None, x[0])\n",
    "    inp_prices.append(cur_llm.price_per_M_input_tokens)\n",
    "    out_prices.append(cur_llm.price_per_M_output_tokens)\n",
    "agg_eval[(\"USD / 1M tokens\", \"Input\")] = inp_prices\n",
    "agg_eval[(\"USD / 1M tokens\", \"Output\")] = out_prices\n",
    "\n",
    "cols_of_interest = [\n",
    "            ('n_invented_tools',              'sum'),\n",
    "            (        'accuracy',                '%'),\n",
    "            (           'score',                '%'),\n",
    "            (\"USD / 1M tokens\",  'Input'),\n",
    "            (\"USD / 1M tokens\", 'Output')\n",
    "]\n",
    "\n",
    "agg_eval = agg_eval[cols_of_interest].copy()\n",
    "\n",
    "agg_eval[('accuracy','%')] = agg_eval[('accuracy','%')].map(lambda z: np.round(100 * z, 1))\n",
    "agg_eval[('score','%')] = agg_eval[('score','%')].map(lambda z: np.round(100 * z, 1))\n",
    "\n",
    "agg_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f960b5e-ab42-4580-8063-117d8b0289a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export for readme\n",
    "markdown_tbl = agg_eval.to_markdown()\n",
    "print(markdown_tbl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7823731d-e834-4b9b-b040-8028b90faf42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export for paper\n",
    "latex_tbl = agg_eval.to_latex(float_format=\"{:.2f}\".format,)\n",
    "print(latex_tbl.replace('_', '\\\\_').replace(' - Anthropic', '').replace(' - OpenAI', '').replace(' - Bedrock', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4bb712-8161-40d4-b3ed-8674f52ed56b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3ada4f-7dc9-43e3-938e-581caf39547a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval_models[(df_eval_models.model == 'GPT 4o - OpenAI') & (df_eval_models.n_invented_tools > 0)].iloc[0].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0632ca2d-3395-4c7c-b9f5-3c22f0eddbb0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
